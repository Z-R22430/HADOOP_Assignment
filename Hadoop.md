1.1.Hive数据仓库在Hadoop中起的作用：
        作为一个数据仓库技术，Hive包括解释器、编译器、优化器及执行器等，可以将SQL 语句转化为MapReduce代码，然后对代码进行编译，最后优化执行。其实质是一个客户端程序，类似jsp和servlet的关系，实际上jsp也是被转化为servlet的。Hive运行时，元数据是存储在一个关系型数据库里面的。
       Hive是建立在Hadoop之上的一种数据仓库基础构架，可以存储、查询和分析存储在Hadoop中的大规模数据。它提供了一系列的工具，可以用来进行数据的提取、转化及加载（ETL).Hive定义了简单的类SQL查询语言，称为HQL,它允许熟悉SQL的用户查询数据。同时，这个语言也允许熟悉MapReduce的开发者开发自定义的Mapper和Reduce,以处理内建的Mapper和Reduce无法完成的复杂的分析工作。Hive没有专门的数据格式，可以很好地工作在Thrift之上，控制分隔符，也允许用户指定数据格式。
       作为Hadoop的数据仓库处理工具，Hive所有的数据都存储在Hadoop兼容的文件系统中。Hive 在加载数据过程中不会对数据进行任何的修改，只是将数据移动到HDFS中Hive 设定的目录下，因此，Hive不支持对数据的改写和添加，所有的数据都是在加载的时候确定的。

2.Hive清洗数据清洗的特点：

​        数据清洗数据清洗的任务是过滤那些不符合要求的数据，将过滤的结果交给业务主管部门，确认是否过滤掉还是由业务单位修正之后再进行抽取。不符合要求的数据主要有不完整的数据、错误的数据及重复的数据3大类。

​         不完整的数据：这一类数据主要是一些应该有的信息缺失，例如，供应商的名称、分公司的名称、客户的区域信息缺失、业务系统中主表与明细表不能匹配等。对于这一类数据过滤出来，按缺失的内容分别写入不同Excel文件向客户提交，要求在规定的时间内补全。补全后才写人数据仓库。

​        错误的数据：这一类错误产生的原因是业务系统不够健全，在接收输入后没有进行判断直接写入后台数据库造成的，例如，数值数据输成全角数字字符、字符串数据后面有一个回车操作、日期格式不正确、日期越界等。这一类数据也要分类，对于类似于全角字符、数据前后有不可见字符的问题，只能通过写SQL语句的方式找出来，然后要求客户在业务系统修正之后抽取。日期格式不正确的或者是日期越界的这一类错误会导致ETL运行失败，这一类错误需要去业务系统数据库用SQL的方式挑出来，交给业务主管部门要求限期修正，修正之后再抽取。重复的数据：对于这一类数据－特别是维表中会出现这种情况－将重复数据记录的所有字段导出来，让客户确认并整理。

​        数据清洗是一个反复的过程，不可能在几天内完成，必须不断地发现问题，解决问题。对于是否过滤，是否修正一般要求客户确认，对于过滤掉的数据，写入Excel文件或者将过滤数据写入数据表，在ETL开发的初期可以每天向业务单位发送过滤数据的邮件，促使他们尽快地修正错误，同时也可以作为将来验证数据的依据。数据清洗需要注意的是，不要将有用的数据过滤掉，对于每个过滤规则认真进行验证，并需要用户确认。

3.Hive元数据库是用来做：本质上只是用来存储hive中有哪些数据库，哪些表，表的模式，目录，分区，索引以及命名空间。为数据库创建的目录一般在hive数据仓库目录下。元数据包含用Hive创建的database、table等的元信息。元数据存储在关系型数据库中。如Derby、MySQL等。

